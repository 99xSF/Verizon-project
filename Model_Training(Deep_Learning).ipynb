{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries (if necessary)\n",
        "!pip install tensorflow\n",
        "\n",
        "# Import Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load Data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "processed_data = pd.read_csv('/content/drive/MyDrive/Verizon 1 2024-2025/Model Process/FE/processed_data.csv')\n",
        "categories_data = pd.read_csv('/content/drive/MyDrive/Verizon 1 2024-2025/categories.csv')\n",
        "sub_categories_data = pd.read_csv('/content/drive/MyDrive/Verizon 1 2024-2025/sub-categories.csv')\n",
        "\n",
        "# Assign Categories and Subcategories based on Description Matching\n",
        "product_descriptions = processed_data['Cleaned Description'].fillna('')\n",
        "category_definitions = categories_data['definition'].fillna('')\n",
        "subcategory_definitions = sub_categories_data['definition'].fillna('')\n",
        "\n",
        "# Combine descriptions for matching\n",
        "all_text = pd.concat([product_descriptions, category_definitions, subcategory_definitions], axis=0)\n",
        "\n",
        "# TF-IDF vectorizer for similarity matching\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(all_text)\n",
        "\n",
        "# Split TF-IDF matrix for each group\n",
        "product_matrix = tfidf_matrix[:len(product_descriptions)]\n",
        "category_matrix = tfidf_matrix[len(product_descriptions):len(product_descriptions) + len(category_definitions)]\n",
        "subcategory_matrix = tfidf_matrix[len(product_descriptions) + len(category_definitions):]\n",
        "\n",
        "# Calculate similarity and assign best match\n",
        "best_category_indices = cosine_similarity(product_matrix, category_matrix).argmax(axis=1)\n",
        "best_subcategory_indices = cosine_similarity(product_matrix, subcategory_matrix).argmax(axis=1)\n",
        "\n",
        "# Map 'name' column in categories_data and sub_categories_data to processed_data\n",
        "processed_data['category'] = [categories_data.iloc[i]['name'] for i in best_category_indices]\n",
        "processed_data['subcategory'] = [sub_categories_data.iloc[i]['name'] for i in best_subcategory_indices]\n",
        "\n",
        "# Text tokenization and padding\n",
        "max_vocab_size = 10000\n",
        "max_sequence_length = 100\n",
        "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(processed_data['Cleaned Description'])\n",
        "\n",
        "# Convert text to sequences and pad them\n",
        "X = tokenizer.texts_to_sequences(processed_data['Cleaned Description'])\n",
        "X = pad_sequences(X, maxlen=max_sequence_length)\n",
        "\n",
        "# Label encode the 'category' column using names from categories_data\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(processed_data['category'])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the Deep Learning Model\n",
        "model = Sequential([\n",
        "    Embedding(max_vocab_size, 128, input_length=max_sequence_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    GlobalAveragePooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#  Train the Model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the Model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = y_pred.argmax(axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "classification_rep = classification_report(y_test, y_pred_classes, labels=range(len(label_encoder.classes_)), target_names=label_encoder.classes_, zero_division=1)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", classification_rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iati33C875JU",
        "outputId": "a89bfbd8-7c02-4438-e579-0d3d4764f64c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.3)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 169ms/step - accuracy: 0.0871 - loss: 2.5390 - val_accuracy: 0.1271 - val_loss: 2.5319\n",
            "Epoch 2/10\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 134ms/step - accuracy: 0.1391 - loss: 2.4874 - val_accuracy: 0.1271 - val_loss: 2.5070\n",
            "Epoch 3/10\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 198ms/step - accuracy: 0.1664 - loss: 2.4493 - val_accuracy: 0.1472 - val_loss: 2.4839\n",
            "Epoch 4/10\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 132ms/step - accuracy: 0.1551 - loss: 2.4734 - val_accuracy: 0.1973 - val_loss: 2.4177\n",
            "Epoch 5/10\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 157ms/step - accuracy: 0.2229 - loss: 2.3502 - val_accuracy: 0.1906 - val_loss: 2.4045\n",
            "Epoch 6/10\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 134ms/step - accuracy: 0.2349 - loss: 2.2479 - val_accuracy: 0.2107 - val_loss: 2.3684\n",
            "Epoch 7/10\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 184ms/step - accuracy: 0.2625 - loss: 2.1125 - val_accuracy: 0.2274 - val_loss: 2.4486\n",
            "Epoch 8/10\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 132ms/step - accuracy: 0.2381 - loss: 2.2030 - val_accuracy: 0.1973 - val_loss: 2.3610\n",
            "Epoch 9/10\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 187ms/step - accuracy: 0.3243 - loss: 1.9880 - val_accuracy: 0.2542 - val_loss: 2.2994\n",
            "Epoch 10/10\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 139ms/step - accuracy: 0.3994 - loss: 1.7902 - val_accuracy: 0.2241 - val_loss: 2.3165\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step\n",
            "Accuracy: 0.22408026755852842\n",
            "Classification Report:\n",
            "                                       precision    recall  f1-score   support\n",
            "\n",
            "                             AI & ML       0.21      0.11      0.14        28\n",
            "                 BI & Data Analytics       0.51      0.46      0.49        39\n",
            "      Customer Operational Platforms       1.00      0.00      0.00        21\n",
            "      Design, Development & Delivery       0.19      0.55      0.28        38\n",
            "                   Digital Workplace       0.06      0.08      0.07        26\n",
            "               Emerging Technologies       1.00      1.00      1.00         0\n",
            "                Enterprise Platforms       0.60      0.15      0.24        20\n",
            "Geographic Information Systems (GIS)       0.05      0.09      0.07        11\n",
            " IT Infrastructure Software Services       1.00      0.00      0.00        14\n",
            "             Management & Governance       0.00      0.00      0.00        29\n",
            "                Marketing Management       0.34      0.65      0.45        23\n",
            "   Performance, Monitoring & Logging       1.00      0.00      0.00        32\n",
            "          Telecom Network Management       0.12      0.22      0.16        18\n",
            "\n",
            "                            accuracy                           0.22       299\n",
            "                           macro avg       0.47      0.25      0.22       299\n",
            "                        weighted avg       0.42      0.22      0.18       299\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(processed_data.columns)\n"
      ],
      "metadata": {
        "id": "nmD9VeNS7M5d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}